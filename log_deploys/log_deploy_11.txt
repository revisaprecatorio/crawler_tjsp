🎯 PROBLEMA ENCONTRADO!
❌ Erro Identificado:
SessionNotCreatedException: Message: session not created: 
probably user data directory is already in use, 
please specify a unique value for --user-data-dir argument, 
or don't use --user-data-dir
🔍 Causa Raiz:
O diretório /app/chrome_profile está sendo compartilhado entre múltiplas execuções do crawler. Quando o orchestrator processa múltiplos processos sequencialmente, cada instância do Selenium tenta usar o mesmo user-data-dir, causando conflito.

✅ Solução: Usar Diretório Único por Processo
Precisamos modificar o orchestrator para criar um user-data-dir único para cada execução:

orchestrator_subprocess.py
+7
-1
~/…/crawler_tjsp$ git add orchestrator_subprocess.py


~/…/crawler_tjsp$ git commit -m "fix: resolve chrome user-data-dir conflict

- Cria user-data-dir único para cada execução do crawler
- Adiciona timestamp e índices para garantir unicidade
- Resolve erro: 'user data directory is already in use'
- Permite processamento paralelo/sequencial sem conflitos"


[main 9cce20c] fix: resolve chrome user-data-dir conflict
 1 file changed, 7 insertions(+), 1 deletion(-)
Exit Code 0
~/…/crawler_tjsp$ git push origin main


persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % git remote set-url origin https://[TOKEN_REDACTED]@github.com/revisaprecatorio/crawler_tjsp.git
persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % git push origin main
Enumerating objects: 5, done.
Counting objects: 100% (5/5), done.
Delta compression using up to 12 threads
Compressing objects: 100% (3/3), done.
Writing objects: 100% (3/3), 9.46 KiB | 9.46 MiB/s, done.
Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)
remote: Resolving deltas: 100% (1/1), completed with 1 local object.
To https://github.com/revisaprecatorio/crawler_tjsp.git
ggit add orchestrator_subproces ss.py % persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % ggit commit -m "fix: adiciona o uutput do crawler nos logs do orchestrator dquote> dquote> -- Imprime stdout do crawler_full.py para debug dquote> -- Mostra stdout também em caso de erro dquote> -- Permite ver mensagens de erro do Selenium/Chromium dquote> -- Facilita troubleshooting de problemas no crawler" [main 7ac6755] fix: adiciona output do crawler nos logs do orchestrator 1 file changed, 10 insertions(+) % persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % ggit push origin main Enumerating objects: 5, done. Counting objects: 20% (1/5)Counting objects: 40% (2/5)Counting objects: 60% (3/5)Counting objects: 80% (4/5)Counting objects: 100% (5/5)Counting objects: 100% (5/5), done. Delta compression using up to 12 threads Compressing objects: 33% (1/3)Compressing objects: 66% (2/3)Compressing objects: 100% (3/3)Compressing objects: 100% (3/3), done. Writing objects: 33% (1/3)Writing objects: 66% (2/3)Writing objects: 100% (3/3)Writing objects: 100% (3/3), 613 bytes | 613.00 KiB/s, done. Total 3 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0) remote: Resolving deltas: 0% (0/2)remote: Resolving deltas: 50% (1/2)remote: Resolving deltas: 100% (2/2)remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To https://github.com/revisaprecatorio/crawler_tjsp.git 2390a53..7ac6755 main -> main % persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % ggit add orchestrator_subproces ss.py % persivalballeste@MacBook-Pro-de-Persival-2 crawler_tjsp % Usar o buffer acessível ⌥F2 examinar manualmente a saídaggit commit -m "fix: resolve ch rrome user-data-dir conflict
✅ CORREÇÃO APLICADA!
🔧 O Que Foi Corrigido:
Problema:
python
# ANTES: Todos usavam o mesmo diretório
--user-data-dir /app/chrome_profile
Solução:
python
# AGORA: Cada execução tem seu próprio diretório
unique_profile = f"{chrome_profile_path}_{job_id}_{i}_{int(time.time())}"
--user-data-dir /app/chrome_profile_27_0_1727760078

----  NOVA EXEC ----

root@srv987902:/opt/crawler_tjsp# docker compose down
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ✔ Container tjsp_worker_1       Removed                                         0.0s 
 ✔ Network crawler_tjsp_default  Removed                                         0.1s 
root@srv987902:/opt/crawler_tjsp# git pull origin main
remote: Enumerating objects: 5, done.
remote: Counting objects: 100% (5/5), done.
remote: Compressing objects: 100% (1/1), done.
remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)
Unpacking objects: 100% (3/3), 649 bytes | 649.00 KiB/s, done.
From https://github.com/revisaprecatorio/crawler_tjsp
 * branch            main       -> FETCH_HEAD
   7ac6755..9cce20c  main       -> origin/main
Updating 7ac6755..9cce20c
Fast-forward
 orchestrator_subprocess.py | 8 +++++++-
 1 file changed, 7 insertions(+), 1 deletion(-)
root@srv987902:/opt/crawler_tjsp# docker compose build
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
#1 [internal] load local bake definitions
#1 reading from stdin 496B done
#1 DONE 0.0s

#2 [internal] load build definition from Dockerfile
#2 transferring dockerfile: 1.19kB done
#2 DONE 0.0s

#3 [internal] load metadata for docker.io/library/python:3.12-slim-bookworm
#3 DONE 0.4s

#4 [internal] load .dockerignore
#4 transferring context: 2B done
#4 DONE 0.0s

#5 [1/7] FROM docker.io/library/python:3.12-slim-bookworm@sha256:25c2d6d3b4680597f42caabe8b3ade05a60bba86547cb219356d5774ab319a8a
#5 DONE 0.0s

#6 [internal] load build context
#6 transferring context: 4.23MB 0.0s done
#6 DONE 0.1s

#7 [4/7] COPY requirements.txt /app/
#7 CACHED

#8 [5/7] RUN pip install --upgrade pip
#8 CACHED

#9 [3/7] WORKDIR /app
#9 CACHED

#10 [2/7] RUN apt-get update && apt-get install -y --no-install-recommends     wget ca-certificates gnupg2 unzip fonts-liberation     libnss3-tools openssl chromium     libnss3 libxss1 libasound2 libatk1.0-0 libgtk-3-0 libgbm1 libx11-xcb1   && rm -rf /var/lib/apt/lists/*
#10 CACHED

#11 [6/7] RUN pip install -r requirements.txt
#11 CACHED

#12 [7/7] COPY . /app/
#12 DONE 0.4s

#13 exporting to image
#13 exporting layers
#13 exporting layers 0.2s done
#13 writing image sha256:d94f4227c8fcc1facbb789b4025f03b95ecafea66da9f1ee506fac0bc7b7bdc5 done
#13 naming to docker.io/library/tjsp-worker:latest done
#13 DONE 0.2s

#14 resolving provenance for metadata file
#14 DONE 0.0s
[+] Building 1/1
 ✔ tjsp-worker:latest  Built                                                     0.0s 
root@srv987902:/opt/crawler_tjsp# psql -h 72.60.62.124 -p 5432 -U admin -d n8n -c "UPDATE consultas_esaj SET status = FALSE WHERE id = 27;"
Password for user admin: 
UPDATE 1
root@srv987902:/opt/crawler_tjsp# docker compose up
WARN[0000] /opt/crawler_tjsp/docker-compose.yml: the attribute `version` is obsolete, it will be ignored, please remove it to avoid potential confusion 
[+] Running 2/2
 ✔ Network crawler_tjsp_default  Created                                         0.1s 
 ✔ Container tjsp_worker_1       Created                                         0.0s 
Attaching to tjsp_worker_1
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o próximo item da fila...
tjsp_worker_1  | Item encontrado para processar: ID=27, CPF=07620857893
tjsp_worker_1  | Conexão com o banco de dados fechada.
tjsp_worker_1  | 
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Processando item 1/1 do Job ID=27: Processo 0077044-50.2023.8.26.0500
tjsp_worker_1  | ================================================================================
tjsp_worker_1  | Executando comando: /usr/local/bin/python crawler_full.py --doc 0077044-50.2023.8.26.0500 --abrir-autos --baixar-pdf --turbo-download --download-dir /app/downloads/07620857893 --user-data-dir /app/chrome_profile_27_0_1759297530
tjsp_worker_1  | 
tjsp_worker_1  | --- Output do Crawler ---
tjsp_worker_1  | {
tjsp_worker_1  |   "documento": null,
tjsp_worker_1  |   "processo": "0077044-50.2023.8.26.0500",
tjsp_worker_1  |   "ok": false,
tjsp_worker_1  |   "has_precatorio": false,
tjsp_worker_1  |   "found_process_numbers": [],
tjsp_worker_1  |   "results": [],
tjsp_worker_1  |   "error": "SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x6521e2bbffba <unknown>\n#1 0x6521e26446d0 <unknown>\n#2 0x6521e267f475 <unknown>\n#3 0x6521e267b368 <unknown>\n#4 0x6521e26ca280 <unknown>\n#5 0x6521e26c9946 <unknown>\n#6 0x6521e26bbc03 <unknown>\n#7 0x6521e26887a8 <unknown>\n#8 0x6521e2689421 <unknown>\n#9 0x6521e2b84b28 <unknown>\n#10 0x6521e2b8887f <unknown>\n#11 0x6521e2b6cc49 <unknown>\n#12 0x6521e2b89405 <unknown>\n#13 0x6521e2b524ff <unknown>\n#14 0x6521e2bad258 <unknown>\n#15 0x6521e2bad432 <unknown>\n#16 0x6521e2bbefa3 <unknown>\n#17 0x7b86372961f5 <unknown>\n\nTraceback (most recent call last):\n  File \"/app/crawler_full.py\", line 1224, in go_and_extract\n    driver = _build_chrome(\n             ^^^^^^^^^^^^^^\n  File \"/app/crawler_full.py\", line 189, in _build_chrome\n    d = webdriver.Chrome(options=opts); d.set_page_load_timeout(60); return d\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chrome/webdriver.py\", line 45, in __init__\n    super().__init__(\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/chromium/webdriver.py\", line 66, in __init__\n    super().__init__(command_executor=executor, options=options)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 212, in __init__\n    self.start_session(capabilities)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 299, in start_session\n    response = self.execute(Command.NEW_SESSION, caps)[\"value\"]\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py\", line 354, in execute\n    self.error_handler.check_response(response)\n  File \"/usr/local/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n    raise exception_class(message, screen, stacktrace)\nselenium.common.exceptions.SessionNotCreatedException: Message: session not created: probably user data directory is already in use, please specify a unique value for --user-data-dir argument, or don't use --user-data-dir\nStacktrace:\n#0 0x6521e2bbffba <unknown>\n#1 0x6521e26446d0 <unknown>\n#2 0x6521e267f475 <unknown>\n#3 0x6521e267b368 <unknown>\n#4 0x6521e26ca280 <unknown>\n#5 0x6521e26c9946 <unknown>\n#6 0x6521e26bbc03 <unknown>\n#7 0x6521e26887a8 <unknown>\n#8 0x6521e2689421 <unknown>\n#9 0x6521e2b84b28 <unknown>\n#10 0x6521e2b8887f <unknown>\n#11 0x6521e2b6cc49 <unknown>\n#12 0x6521e2b89405 <unknown>\n#13 0x6521e2b524ff <unknown>\n#14 0x6521e2bad258 <unknown>\n#15 0x6521e2bad432 <unknown>\n#16 0x6521e2bbefa3 <unknown>\n#17 0x7b86372961f5 <unknown>\n\n",
tjsp_worker_1  |   "downloaded_files": [],
tjsp_worker_1  |   "started_at": "2025-10-01 02:45:30",
tjsp_worker_1  |   "finished_at": "2025-10-01 02:45:31",
tjsp_worker_1  |   "last_url": null,
tjsp_worker_1  |   "duration_seconds": 0.954,
tjsp_worker_1  |   "duration_hms": "0m01s"
tjsp_worker_1  | }
tjsp_worker_1  | 
tjsp_worker_1  | --- Fim do Output ---
tjsp_worker_1  | 
tjsp_worker_1  | 
tjsp_worker_1  | Atualizando status para 'processado = TRUE' para o ID: 27
tjsp_worker_1  | [SUCESSO] Status atualizado para o ID 27.
tjsp_worker_1  | Conectando ao banco de dados PostgreSQL...
tjsp_worker_1  | Executando a query para buscar o próximo item da fila...
tjsp_worker_1  | Conexão com o banco de dados fechada.
tjsp_worker_1  | Nenhum precatório novo para processar encontrado. Encerrando o worker.
tjsp_worker_1 exited with code 0
root@srv987902:/opt/crawler_tjsp# 

w Enable Watch

